当前项目里目前我们已经实现了一个，可以让调用者在命令行操作chrome浏览器的流程。具体来说，现在我们可以在cli命令行做到这些操作：
Available commands:

  Shortcut Commands:
    reset                    - Reset mouse to center
    click [left|right|middle] - Click mouse button (default: left)
    move <dx> <dy>          - Move mouse relative
    scroll <up|down|left|right> [amount] - Scroll (default: down, 100)
    type <text>             - Type text
    press <key> [modifiers] - Press special key
    screenshot              - Capture screenshot
    tabs list               - List all tabs
    tabs init <url>         - Initialize new managed session
    tabs open <url>         - Open new tab
    tabs close <tab_id>     - Close tab
    tabs switch <tab_id>    - Switch to tab
这些操作基本已经完全模拟了一个真实用户的行为！基于这些操作我们可以做一件非常激动人心的事情：我们可以做一个多模态大模型AI Agent，我们叫他OpenBrowserAgent！意思是他可以解放我们操作浏览器的行为。

1. 集成openhands sdk。openhands sdk是一套AI Agent脚手架，我相信利用它我们能更快搭建自己的AI Agent。不过呢你要使用https://github.com/softpudding/agent-sdk/tree/switch-litellm-fork这个版本的代码！记得用uv集成。同时这个代码的copy我放在了reference/software-agent-sdk下，供参考。
2. 实现open-browser tool！实现的时候把他放在server/agent/tools目录下，最终我们要把AI Agent集成到当前的server里。（这样他可以直接调用server的能力，不用走弯路）看一看openhands sdk要求的AI tool是怎么实现的（可以参考reference/software-agent-sdk/examples/01_standalone_sdk/02_custom_tools.py）。我们的重点
  - 给AI 提供 reset click （只支持left）move scrll type press tabs相关操作
  - 从tool角度来说，AI每次操作完之后，我们同时返回text content+image content (image content可以参考reference/software-agent-sdk/examples/01_standalone_sdk/17_image_input_base64.py)
  - text content 包含当前全部tab信息，以及鼠标的坐标位置（要用我们的模拟坐标系，而不是真实坐标系）
  - image content 包含 当前AI发起的动作执行完之后的一张截图
  - 工具描述里要写清楚当前提供的图片像素是多少，然后AI应该怎么关注画面操作鼠标
  - 通过这种设计，我们可以让AI助手每次执行完浏览器操作，看到浏览器的实时画面！
3. server层实现 server/agent/agent.py。这是agent的主要逻辑入口，给他配置我们在2实现好的browser tool以及默认的tool set。这个主入口应该做成一个async函数，这样上层可以利用这个函数获得流式的输出（sse）。这个入口返回的函数来自于openhands Conversation类的visualizer回调；我们要在这个文件里实现自己的OpenBrowserAgentVisualizer，然后用这个visualizer实现可视化，返回text及image信息。（对于其他内容，比如用户信息，llm信息，工具文字信息可以参考sdk默认实现，但是对于open-browser的工具返回要确保返回图片的信息，可以是url）。
  - 另外要注意，Conversation是可以配置conversation id的。Agent的对话接口要传Conversation id，这样子才可以支持连贯的多轮对话。
4. server层在 server/api/main.py实现agent调用的后端入口。我们的最终呈现方式是在浏览器里有一个local页面，用户在这个页面和AI Agent对话，执行任务。main.py的主要逻辑就是用户输入语言，然后sse返回Agent visualizer渲染的信息。
5. 完成可视化前端。前端做成一个优雅的对话框形式。